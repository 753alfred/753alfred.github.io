# Reasoning Beyond Language: 잠재 Chain-of-Thought 추론에 대한 포괄적 조사

**논문 출처**: [arXiv:2505.16782](https://arxiv.org/pdf/2505.16782v1)

**저자**: Xinghao Chen, Anhao Zhao, Heming Xia, Xuan Lu, Hanlin Wang, Yanjun Chen, Wei Zhang, Jian Wang, Wenjie Li, Xiaoyu Shen

**소속**: The Hong Kong Polytechnic University, Ningbo Digital Twin Institute

## Abstract

- 대규모 언어 모델(LLM)은 Chain-of-Thought (CoT) 프롬프팅을 통해 복잡한 추론 작업에서 인상적인 성능을 달성함
- 그러나 기존의 CoT는 자연어로 명시적으로 표현된 추론 단계에 의존하여 비효율성을 야기하고 추상적 추론에 대한 적용 가능성을 제한
- 이를 해결하기 위해 추론이 잠재 공간 내에서 발생하는 잠재 CoT 추론에 대한 연구 관심이 증가하는 중
- 추론을 언어로부터 분리함으로써, 잠재 추론은 더 풍부한 인지적 표현과 더 유연하고 빠른 추론을 보장

## 1. Introduction

> "Whereof one cannot speak, thereof one must be silent." — Ludwig Wittgenstein

- 대규모 언어 모델(LLM)은 자연어를 통한 단계별 추론을 장려하는 Chain-of-Thought (CoT) 추론을 통해 복잡한 추론 작업에서 놀라운 능력을 보여줌
- 이 접근법은 해석 가능성을 향상시킬 뿐만 아니라 종종 더 나은 작업 성능으로 이어짐

### 명시적 CoT의 한계

명시적 CoT 추론은 각 단계를 자연어로 표현하는 데 의존함으로써 본질적으로 제약을 받음. 이러한 언어적 매개는 두 가지 주요 문제를 야기함.

1. **계산 비효율성**: 표현된 사고 과정의 모든 토큰이 정보적 내용을 담지 않기 때문에 계산 비효율성을 초래함

2. **언어의 한계**: 인간의 사고는 종종 언어의 한계를 초월함. 추상적 통찰, 직관적 도약, 또는 고도로 구성적인 사고와 같은 인지의 다른 측면들은 완전하거나 정확한 언어화에 저항함

![Explicit CoT vs Latent CoT](https://arxiv.org/html/2505.16782v1/x1.png)

**Figure 1**: Explicit CoT (왼쪽)는 자연어로 추론 단계를 생성하고, latent CoT (오른쪽)는 모델이 잠재 공간에서 내부적으로 추론하도록 함

### Latent CoT의 등장

이러한 한계를 해결하기 위해 **잠재 Chain-of-Thought 추론**이라는 새로운 패러다임이 등장함. 잠재 CoT에서는 추론 과정이 자연어 출력 없이 모델의 내부 잠재 공간에서 수행됨.

![Latent CoT 분류 체계]()

**Figure 2**: Latent Chain-of-Thought (CoT) 추론의 분류 체계

## 2. Overview

본 논문은 잠재 CoT 추론에 대한 포괄적인 개요와 분석을 제공함. 네 가지 관점에서 통합된 분류 체계를 제안함.

1. **토큰 단위 전략 (Token-wise Strategies)**
2. **내부 메커니즘 (Internal Mechanisms)**
3. **분석 및 해석 가능성 (Analysis and Interpretability)**
4. **응용 (Applications)**

각 범주의 대표적인 방법들에 대한 심층적인 논의와 비교 분석을 제공하여, 설계 패턴, 강점, 그리고 열린 도전 과제들을 강조함.

## 3. Token-wise Strategies

토큰 단위 전략은 특수한 토큰을 활용하여 중간 추론을 표현하거나 유도하는 방법들임.

### 3.1 Discrete Tokens

이산 토큰 방식은 사람이 읽을 수 있는 기호나 마커 토큰을 이용해 모델의 사고 과정을 명시적으로 구분하는 방법임.

**주요 기법들**

-   **Pause Tokens**: `[PAUSE]`, `[THINK]` 같은 특수 마커 사용
-   **Planning Tokens**: 계획 수립을 위한 특별 토큰
-   **Thinking Tokens**: 사고 과정을 나타내는 토큰
-   **Quiet-STaR**: 학습된 특수 토큰으로 숨겨진 추론의 시작과 끝을 표시

**장점**

-   구현이 간단
-   이해하기 용이
-   즉시 적용 가능

**단점**

-   여전히 토큰 생성 필요
-   일부 비효율성 잔존

### 3.2 Continuous Tokens

연속 토큰 방식은 학습된 연속 벡터 표현을 "생각을 담는 토큰"으로 활용함.

![연속 토큰 방식](https://arxiv.org/html/2505.16782v1/x2.png)

**Figure 3**: 대표적인 연속 토큰 기반 방법들의 설명. 내재적 방법들은 단일 LLM 내에서 연속 토큰을 생성하고 소비함. 보조적 방법들은 외부 모듈을 사용하여 연속 토큰을 생성함

**주요 기법들**

**내재적 방법 (Intrinsic Methods)**

-   **COCONUT**: 모델의 마지막 은닉상태를 다시 입력으로 피드백
-   **CODI**: 교사 모델의 CoT 추론 패턴을 학생 모델에 자기-증류
-   **LightThinker**: 사고 내용을 압축하여 핵심만 요약 토큰으로 저장

**보조적 방법 (Auxiliary Methods)**

-   **HCoT**: 보조 CoT 모델이 전체 사고 과정을 압축된 벡터로 표현
-   **CCoT**: 가변 길이 잠재 임베딩 시퀀스로 전체 추론 과정 인코딩
-   **SoftCoT**: 프롬프트 튜닝 기법으로 소프트 토큰 생성
-   **SoftCoT++**: 테스트 시 다양한 연속 토큰 생성으로 연속 공간 탐색

## 4. Internal Mechanisms

내부 메커니즘 기반 전략은 모델의 내부 구조나 학습 방법을 개조하여 잠재 CoT를 구현함.

### 4.1 Structural CoT

구조적 CoT는 모델의 아키텍처 자체를 변경하여 여러 단계의 추론이 자연스럽게 이루어지도록 함.

![구조적 CoT](https://arxiv.org/html/2505.16782v1/x3.png)

**Figure 4**: 구조적 CoT 메커니즘의 설명. 순환 모듈을 통한 은닉 상태의 반복적 개선을 통해 잠재 추론이 나타남. 기존 연구들은 일반적으로 각 순환을 CoT의 개별 추론 단계로 해석함

**주요 기법들**

-   **CoTFormer**: Transformer 내부에서 표현을 반복하여 다단계 추론 효과
-   **Huginn**: RNN처럼 동적으로 깊이를 조절
-   **RELAY**: 반복 루프 1회당 하나의 추론 단계에 대응
-   **ITT (Inner Thinking Transformer)**: 각 Transformer 층을 독립된 추론 단계로 취급
-   **Looped Transformers**: 루프 구조를 통한 반복 추론

**핵심 개념**

-   **딥 씽킹**: 같은 정보를 반복 숙고하여 더 깊은 추론 달성
-   **순환 구조**: 동일 네트워크를 여러 번 통과시켜 단계별 추론
-   **파라미터 효율성**: 추가 파라미터 없이 깊이 증가

### 4.2 Representational CoT

표현적 CoT는 모델의 학습 기법을 활용하여 명시적 CoT의 효과를 모델의 은닉 표현에 내재화함.

**주요 기법들**

-   **STaR**: 자기 해설을 재학습하여 추론 능력 내재화
-   **ICoT**: 모델 매개변수에 추론 능력 저장
-   **단계적 내재화**: 교사-학생 학습을 통한 추론 지식 전수
-   **System-2 Distillation**: 대형 모델의 추론 행태를 소형 모델에 증류

**목표**

-   추론 능력을 모델 자체에 내재화
-   명시적 단계 없이 내적 추론 수행
-   극단적 형태의 잠재 CoT 구현

## 5. Analysis and Interpretability

잠재 CoT의 내부 작동을 분석하고 이해하려는 연구들임.

### 5.1 Internal Computation Interpretation

모델이 내부적으로 진짜 추론을 수행하고 있다는 증거를 찾는 연구들

**주요 발견들**

-   **어텐션 패턴 분석**: 질문에 대한 추론 트리가 내부적으로 형성됨
-   **레이어 패턴**: 일부 레이어가 이전 레이어 출력을 재사용하여 반복 추론
-   **은닉 상태 분석**: 여러 추론 경로가 병렬로 표현될 수 있음
-   **그로킹 현상**: 충분한 학습 후 내부에 단계적 추론 회로 형성
-   **다중 홉 추론**: 모델이 자동으로 중간 다리 사실을 검색

### 5.2 Shortcut Mechanisms

모델이 실제 추론 대신 지름길을 사용할 수 있다는 의심 연구들

**주요 발견들**

-   **조기 답변**: 초기 레이어에서 이미 정답이 암시되는 현상
-   **전문가 흉내**: 실제 추론 단계를 건너뛰고 바로 답으로 점프
-   **스푸리어스 상관관계**: 토큰 간 허위 상관관계에 의존
-   **동적 전략**: 문제 난이도에 따라 추론과 지름길을 오가는 행동

### 5.3 Latent Reasoning Dynamics

잠재 공간에서의 추론 과정을 이해하고 제어하려는 연구들

**주요 기법들**

-   **인과적 개입**: 난이도에 따른 추론 모드 전환 확인
-   **잠재 CoT 벡터**: 특별한 방향으로 CoT 스타일 사고 유도
-   **CoE (Chain-of-Thought Embedding)**: 시간에 따른 은닉 상태 궤적 표현
-   **자기 평가**: 모델이 스스로 추론을 평가하도록 유도

## 6. Applications

| 응용 분야                                         | 주요 응용                                                                                                                                                                                                                          | 성과/장점/효과                                                                                                        |
| ------------------------------------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | --------------------------------------------------------------------------------------------------------------------- |
| Textual Reasoning                                 | • 수학 문제 풀이<br/>• 상식 추론<br/>• 논리 퍼즐<br/>• 코드 추론                                                                                                                                                                   | • 토큰 수 감소와 동시에 성능 유지 또는 향상<br/>• 복잡한 다단계 문제 해결 능력 개선                                   |
| Multimodal Reasoning and Generation               | • Heima: 멀티모달 문제에서 중간 과정을 잠재 공간에 요약<br/>• XS-CoT: 음성 언어 간 추론에 반-암시적 형태 적용<br/>• LatentLM: 모든 modality를 잠재 토큰으로 통합 처리                                                              | • 모든 중간 추론을 언어로 설명할 필요 없음<br/>• 다양한 정보 형태의 자연스러운 통합<br/>• 멀티모달 지능의 효율성 향상 |
| Retrieval-Augmented Generation and Recommendation | • 검색 기반 생성 (RAG): 검색-추론 단계를 압축하여 지연 시간 감소<br/>• 가상 토큰: 외부 지식과 추론을 전달하는 경량 컨테이너<br/>• DEBATER: Chain-of-Deliberation으로 정보 검색 개선<br/>• ReaRec: 추천 시스템에서 사용자 선호 추론 | • 사용자에게 중간 과정 노출 불필요<br/>• 빠르고 깔끔한 반응으로 사용자 경험 향상                                      |

## 7. Challenges and Future Directions

### 7.1 Challenges

| 항목                      | 문제점                                                                                                                                                                      | 해결 방향                                                                                         |
| ------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------- |
| Training Difficulties     | • 정답에 대한 감독만 있고 내부 잠재 추론 과정 자체를 직접 감독하지 못함<br/>• 모델 내부의 사고를 활성화하는 충분한 신호 부족<br/>• 잠재 CoT의 잠재력이 완전히 발휘되지 않음 | • 중간 사고의 옳고 그름까지 평가하여 보상<br/>• 모델이 자체 생각을 검증하도록 훈련                |
| Generalization Issues     | • 훈련된 패턴에는 안정적이지만 다른 유형 문제에서 성능 저하<br/>• 잠재 공간에서 특정 템플릿 압축에 그쳐 융통성 부족<br/>• 틀에 박힌 "생각 흉내"일 가능성                    | • 더 다양한 추론 데이터 활용<br/>• 추상적 수준에서의 추론 학습<br/>• 메타학습 등 일반화 기술 적용 |
| Interpretability Concerns | • 추론 과정 생략으로 인한 불투명성<br/>• 모델의 내부 사고 과정을 알기 어려움<br/>• 디버깅과 신뢰성 판단의 어려움                                                            | • 투명성과 검증 가능성 확보<br/>• 모델 내부 사고의 일부라도 복원 가능한 기술 개발                 |

### 7.2 Future Directions

| 항목                                   | 연구 방향                                                                                                                                                       | 기대 효과                                                                                             |
| -------------------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------- | ----------------------------------------------------------------------------------------------------- |
| Alternative Architectures              | • Transformer 외 잠재 추론에 유리한 새로운 모델 구조<br/>• 디퓨전 모델을 활용한 비순차적이고 전역적인 사고<br/>• 순환/루프형 Transformer의 발전                 | • 표현력과 효율성의 동시 향상<br/>• 더 자연스러운 잠재 추론 구현                                      |
| Interpretability and Verification      | • 잠재 상태를 해독하여 추론 흐름 추출<br/>• 결과의 신뢰도 사전 점검 방법<br/>• 프로빙 기법과 역변환 기술 개발<br/>• 모델의 내부 추론과 최종 답의 교차검증       | • 신뢰할 수 있는 "생각 표시기" 구비<br/>• 설명 가능하고 안전한 시스템 구축                            |
| Training Approaches                    | • 강화학습: 내부 추론에 대한 보상 함수 정의<br/>• 자기 진화: 보상에 따라 구조화된 추론 공간 형성<br/>• 커리큘럼 러닝: 쉬운 추론부터 어려운 추론까지 단계적 훈련 | • 모델의 잠재 공간 형성에 직접 개입<br/>• 잠재 CoT 능력 극대화                                        |
| LLM Agents                             | • 자율 에이전트형 LLM의 효율성 향상<br/>• 길고 복잡한 계획 수립을 내부적으로 수행<br/>• 더 짧고 빠른 결정 과정                                                  | • 에이전트의 실행 효율 향상<br/>• 불필요한 출력 혼선 감소<br/>• 더 나은 사용자 경험                   |
| Social Intelligence and Theory of Mind | • 사회적 상호작용에서의 잠재 추론<br/>• 타인의 의도 추론 능력 (Theory of Mind)<br/>• 중첩된 정신 상태의 내부 시뮬레이션                                         | • 사회적으로 유능한 AI 개발<br/>• 인간의 복잡한 감정과 의도 추론<br/>• 불필요한 설명 없이 적절한 대응 |

## 8. Conclusion

### 주요 기여

본 논문은 잠재 Chain-of-Thought 추론에 대한 첫 번째 포괄적인 조사를 제공함.

1. **체계적 분류**: 네 가지 관점에서 통합된 분류 체계 제안
2. **심층 분석**: 각 방법의 설계 패턴, 강점, 한계 분석
3. **미래 방향**: 구체적인 연구 아이디어와 발전 방향 제시

### 핵심 메시지

**잠재 CoT의 가치**

-   언어를 넘어선 추론으로 AI의 사고 방식 혁신
-   효율성과 표현력의 동시 향상
-   더 추상적이고 확장 가능한 추론 가능

**미래 전망**

-   진정한 "생각하는 기계" 구현 가능성
-   인간처럼 내적 사고 과정을 보유하는 AI
-   AI의 인지 능력 한 단계 진화

### Limitations

1. **평가의 어려움**: 잠재 추론의 질을 정량적으로 평가하기 어려움
2. **표준화 부족**: 통일된 평가 기준과 벤치마크 필요
3. **이론적 기반**: 잠재 추론의 이론적 토대 더 강화 필요

### Ethics Statement

잠재 CoT 연구는 다음과 같은 윤리적 고려사항을 포함함.

1. **투명성**: 모델의 사고 과정이 불투명해질 위험
2. **신뢰성**: 잘못된 추론 과정을 검증하기 어려운 문제
3. **공정성**: 편향된 잠재 추론이 결과에 영향을 미칠 가능성
4. **책임성**: 잠재 추론 기반 결정의 책임 소재 명확화 필요

---

**참고자료**

-   [논문 원본](https://arxiv.org/html/2505.16782v1)
-   [관련 코드 저장소](https://github.com/EIT-NLP/Awesome-Latent-CoT)

**리뷰 작성일**: 2025년 1월
